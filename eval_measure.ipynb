{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qBoTpZftdxla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Confusion matrix for computing Prec/Recall/Count of predictions (TP/FP)"
      ],
      "metadata": {
        "id": "LMoP-nlNwcTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtqWPHipdhmV"
      },
      "outputs": [],
      "source": [
        "def show_confusionmatrix(y_true,y_pred):\n",
        "\n",
        "    classes = tuple(set(y_pred))\n",
        "\n",
        "    if len(classes) < 4 and max(classes)==3 :\n",
        "      classes = (0,1,2,3)\n",
        "\n",
        "    if len(classes) < 3 and max(classes) < 3:\n",
        "      classes = (0,1,2)\n",
        "    # classes = (0,1,2)\n",
        "\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    df_cm_recall = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                        columns = [i for i in classes])\n",
        "\n",
        "    df_cm_prec = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=0)[None,:], index = [i for i in classes],\n",
        "                        columns = [i for i in classes])\n",
        "\n",
        "    flatten_df_cm_recall = df_cm_recall.to_numpy().flatten().tolist()\n",
        "    flatten_df_cm_prec = df_cm_prec.to_numpy().flatten().tolist()\n",
        "    flatten_cf_matrix = cf_matrix.flatten().tolist()\n",
        "\n",
        "    return flatten_df_cm_recall, flatten_df_cm_prec,  flatten_cf_matrix, len(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Label in COT settings"
      ],
      "metadata": {
        "id": "qBATtOPGbte4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(text_label):\n",
        "    '''\n",
        "    convert label 0.1.2. to text-based label\n",
        "    '''\n",
        "    if 'passive' in text_label.lower():\n",
        "        label = 0\n",
        "    elif 'active' in text_label.lower():\n",
        "        label = 1\n",
        "    elif 'constructive' in text_label.lower() :\n",
        "        label = 2\n",
        "    else:\n",
        "        print('Undefined label found: ', text_label)\n",
        "        label = 3\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def extract_pred_label_COT(response):\n",
        "  '''\n",
        "  When we use the following format (used in pureFew shot)\n",
        "   \"Question:\n",
        "    \"Statement:\n",
        "    \"Label:\n",
        "    \"Chain-of-thought:\n",
        "  '''\n",
        "  splitted_res = response.splitlines()\n",
        "\n",
        "  label_num = 3 #undefined\n",
        "  for text in splitted_res:\n",
        "    if \"label:\" in text.lower():\n",
        "      label_num = label_to_num(text)\n",
        "      break\n",
        "\n",
        "  return label_num"
      ],
      "metadata": {
        "id": "OMryLPLBbzmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accruacy"
      ],
      "metadata": {
        "id": "9_XoUKDfwgt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true,y_pred):\n",
        "    return sum(1 for x,y in zip(y_true,y_pred) if x == y) / float(len(y_true))"
      ],
      "metadata": {
        "id": "g8RXWiy2d2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output to csv for Prec/Recall/Accruacy"
      ],
      "metadata": {
        "id": "y1sul4VZwidp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import compress\n",
        "def create_dataframe(list_2d, num_cls_list, output_csv, gpt_models_all, prompt_vers_all):\n",
        "\n",
        "\n",
        "    data_with_uncategorized = list(compress(list_2d, [k==4 for k in num_cls_list]))\n",
        "    model_uncateg = list(compress(gpt_models_all, [k==4 for k in num_cls_list]))\n",
        "    promt_uncateg = list(compress(prompt_vers_all, [k==4 for k in num_cls_list]))\n",
        "\n",
        "    df_uncateg = pd.DataFrame(data_with_uncategorized, columns=['0_0','0_1','0_2','0_3','1_0','1_1','1_2','1_3','2_0','2_1','2_2','2_3','3_0','3_1','3_2','3_3'])\n",
        "    df_uncateg.insert(0,'model', model_uncateg)\n",
        "    df_uncateg.insert(1,'prompt', promt_uncateg)\n",
        "\n",
        "    data_without_uncategorized = list(compress(list_2d, [k==3 for k in num_cls_list]))\n",
        "    model_without_uncateg = list(compress(gpt_models_all, [k==3 for k in num_cls_list]))\n",
        "    promt_without_uncateg = list(compress(prompt_vers_all, [k==3 for k in num_cls_list]))\n",
        "\n",
        "    df_no_uncateg = pd.DataFrame(data_without_uncategorized, columns=['0_0','0_1','0_2','1_0','1_1','1_2','2_0','2_1','2_2'])\n",
        "    df_no_uncateg.insert(0,'model', model_without_uncateg)\n",
        "    df_no_uncateg.insert(1,'prompt', promt_without_uncateg)\n",
        "\n",
        "    df = pd.concat([df_uncateg, df_no_uncateg], ignore_index=True, sort=False)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df\n",
        "\n",
        "def create_dataframe_acc(acc_list, num_class_all, output_csv, gpt_models_all, prompt_vers_all):\n",
        "\n",
        "    df = pd.DataFrame(acc_list, columns=['accuracy'])\n",
        "    df.insert(0,'model', gpt_models_all)\n",
        "    df.insert(1,'prompt', prompt_vers_all)\n",
        "    df['num_cls'] = num_class_all\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "0whm1ZKJd5Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 score and simplified prec/recall csv\n",
        "Micro F1 Average = Accruacy\n",
        "\n",
        "Macro F1 Average = Average of per-class F1 score\n",
        "\n",
        "Weighted F1 Average = Sum of (per-class F1 score * (actual occurences of the (true) class in the dataset/ total test data))"
      ],
      "metadata": {
        "id": "dgR_9IR1wZVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(precision_csv, recall_csv, count_csv, output_f1_csv, output_prec_recall_simple_csv, num_class = 3):\n",
        "  df_prec = pd.read_csv(precision_csv)\n",
        "  df_recall = pd.read_csv(recall_csv)\n",
        "\n",
        "  df_count = pd.read_csv(count_csv)\n",
        "\n",
        "\n",
        "  perclass_f1_df = df_prec.loc[:, ['model','prompt']]\n",
        "\n",
        "  prec_recall_df =  df_prec.loc[:, ['model','prompt']]\n",
        "\n",
        "  all_count = df_count.drop(['model','prompt'], axis=1).sum(axis=1)\n",
        "\n",
        "  col_perclass_f1_list = []\n",
        "  col_weighted_f1_list = []\n",
        "  for k in range(num_class):\n",
        "\n",
        "    # Get precision and Recall [0_0], [1_1], [2_2]===============\n",
        "    col_name = '{0}_{1}'.format(k,k)\n",
        "\n",
        "\n",
        "    prec = df_prec.loc[:, [col_name]]\n",
        "    recall = df_recall.loc[:, [col_name]]\n",
        "\n",
        "    prec_recall_df[str(k)+'_prec'] = prec\n",
        "    prec_recall_df[str(k)+'_recall'] = recall\n",
        "\n",
        "\n",
        "    #per class f1===========================\n",
        "    perclass_f1 = 2*prec*recall / (prec+recall)\n",
        "\n",
        "    col_name_f1 = '{0}_perclass_f1'.format(k)\n",
        "    col_perclass_f1_list.append(col_name_f1)\n",
        "\n",
        "    perclass_f1_df[col_name_f1] = perclass_f1\n",
        "\n",
        "    #weighted f1===========================\n",
        "\n",
        "    col_list = []\n",
        "    for col in df_count.columns:\n",
        "      if col == 'model' or col == 'prompt':\n",
        "        continue\n",
        "      true_class = col[0]\n",
        "\n",
        "      if int(true_class) == k:\n",
        "        col_list.append(col)\n",
        "\n",
        "    num_data_per_class = df_count[col_list].sum(axis=1)\n",
        "    proportion= num_data_per_class / all_count\n",
        "\n",
        "    col_name_weighted = '{0}_weighted_f1'.format(k)\n",
        "    col_weighted_f1_list.append(col_name_weighted)\n",
        "\n",
        "    perclass_f1_df[col_name_weighted] =  perclass_f1_df[col_name_f1] *  proportion\n",
        "\n",
        "  # Average Skip NA ==========================\n",
        "  perclass_f1_df['macro_average_skipNA'] =  perclass_f1_df[col_perclass_f1_list].mean(axis=1)\n",
        "  perclass_f1_df['weighted_average_skipNA'] =  perclass_f1_df[ col_weighted_f1_list].sum(axis=1)\n",
        "\n",
        "\n",
        "  # Average replace NA with 0==========================\n",
        "  perclass_f1_replace_na_df =  perclass_f1_df.fillna(0)\n",
        "  perclass_f1_df['macro_average_NAas0'] =  perclass_f1_replace_na_df[col_perclass_f1_list].mean(axis=1)\n",
        "  perclass_f1_df['weighted_average_NAas0'] =  perclass_f1_replace_na_df[ col_weighted_f1_list].sum(axis=1)\n",
        "\n",
        "\n",
        "  perclass_f1_df.to_csv(output_f1_csv, index=False)\n",
        "  prec_recall_df.to_csv(output_prec_recall_simple_csv, index=False)\n",
        "\n",
        "  return perclass_f1_df, prec_recall_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5WgV1OcOR5YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Cnibjp_1mUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just for testing"
      ],
      "metadata": {
        "id": "bU3PAfdrwtgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# out_dir = '/content/drive/MyDrive/Classification Model/COT/Evaluation/updated_test_wquestion_9_13_23_preds/updated_test_wquestion_9_13_23_eval_metrics/'\n",
        "\n",
        "out_dir = '/content/drive/MyDrive/Classification Model/COT/COT refinement 9_14_2023/Evaluation_ASST1/updated_test_wquestion_9_13_23_ASST1_preds/'\n",
        "\n",
        "prec_csv = out_dir + 'prec_all.csv'\n",
        "recall_csv = out_dir + 'recall_all.csv'\n",
        "count_csv = out_dir + 'count_all.csv'\n",
        "\n",
        "output_f1_csv = out_dir + 'f1_score.csv'\n",
        "output_prec_recall_simple_csv =  out_dir + 'prec_recall_simple.csv'\n",
        "\n",
        "compute_f1(prec_csv, recall_csv, count_csv, output_f1_csv, output_prec_recall_simple_csv, num_class=3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhHPXJesVG8w",
        "outputId": "9d596fd8-36a0-4e86-8015-329ead3b3207",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   model prompt  0_perclass_f1  0_weighted_f1  1_perclass_f1  1_weighted_f1  \\\n",
              " 0  gpt-4  1a_v2       0.728682       0.383233       0.701493       0.306578   \n",
              " \n",
              "    2_perclass_f1  2_weighted_f1  macro_average_skipNA  \\\n",
              " 0            NaN            NaN              0.715087   \n",
              " \n",
              "    weighted_average_skipNA  macro_average_NAas0  weighted_average_NAas0  \n",
              " 0                 0.689811             0.476725                0.689811  ,\n",
              "    model prompt    0_prec  0_recall    1_prec  1_recall  2_prec  2_recall\n",
              " 0  gpt-4  1a_v2  0.810345  0.661972  0.626667   0.79661     0.0       0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data"
      ],
      "metadata": {
        "id": "Ckj5lEWwf-wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "'''\n",
        "From Google Drive\n",
        "'''\n",
        "drive.mount('/content/drive')\n",
        "# file_path = '/content/drive/MyDrive/Classification Model/COT/Evaluation/mergeddata_withQuestion_9_13_23/2a_v2_gpt-4_index0_to_41.csv' #add the file\n",
        "# df = pd.read_csv(file_path)\n",
        "\n",
        "'''\n",
        "by Upload\n",
        "'''\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv(io.BytesIO(uploaded['EDM_master - EDM_master.csv']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "78e153dc-2501-40b1-bd06-558e24df3caa",
        "id": "fR-fss28TDwV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nby Upload\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall_all = []\n",
        "prec_all = []\n",
        "count_all = []\n",
        "num_class_all = []\n",
        "accuracy_all = []\n",
        "gpt_models_all = []\n",
        "prompt_vers_all  = []\n",
        "\n",
        "# gpt_models = ['gpt-3.5-turbo', 'gpt-4']\n",
        "# prompt_vers = ['1a_v1', '2a_v1','1a_v2', '2a_v2']\n",
        "\n",
        "\n",
        "# index_name  = '_index0_to_４１_v2'\n",
        "index_name  = '_index0_to_110_v0'\n",
        "# index_name  = '_index0_to_134'\n",
        "\n",
        "\n",
        "# prompt_vers = ['1a_v2_assertion_dos_and_donts']\n",
        "\n",
        "# gpt_models = ['gpt-4','gpt-4','gpt-4']\n",
        "\n",
        "\n",
        "gpt_models = ['gpt-4']\n",
        "prompt_vers = [ 'pure_fewshot_specificCOT', 'pure_fewshot_specificCOT_genCOT', 'pure_fewshot_specificCOT_assertion', 'pure_fewshot_specificCOT_genCOT_assertion']\n",
        "# prompt_vers = ['pure_fewshot_specificCOT_genCOT']\n",
        "\n",
        "for i, gpt_model in enumerate(gpt_models):\n",
        "  for prompt_ver in prompt_vers:\n",
        "    try:\n",
        "      pred_dir = '/content/drive/My Drive/Data_ClassificationModel/'\n",
        "\n",
        "      # pred_dir = '/content/drive/MyDrive/Classification Model/COT/Evaluation/mergeddata_withQuestion_9_13_23_preds/csv_files/'\n",
        "      # pred_dir = '/content/drive/MyDrive/Classification Model/COT/Evaluation/updated_test_wquestion_9_13_23_preds/'\n",
        "\n",
        "      # pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name + \"_v\" +str(i) + '.csv'\n",
        "      pred_path =  pred_dir + prompt_ver + \"_\" + gpt_model + index_name + '.csv'\n",
        "      df = pd.read_csv(pred_path)\n",
        "\n",
        "\n",
        "      print(pred_path)\n",
        "      # df = pd.read_csv('/content/drive/MyDrive/Data_ClassificationModel/1a_v2_assertion_gpt-4_index0_to_51.csv')\n",
        "      # df_1 = pd.read_csv('/content/drive/MyDrive/Data_ClassificationModel/1a_v2_assertion_gpt-4_index0_to_51.csv')\n",
        "      # df_2 = pd.read_csv('/content/drive/MyDrive/Data_ClassificationModel/1a_v2_assertion_gpt-4_index51_to_134.csv')\n",
        "      # df = df_1.append(df_2, ignore_index=True)\n",
        "\n",
        "\n",
        "      y_true = df['label'].tolist()\n",
        "\n",
        "      # y_pred = df['pred_label'].tolist()\n",
        "\n",
        "      df['pred_label_fixed'] = df.apply(lambda row: extract_pred_label_COT(row['pred_response']), axis=1)\n",
        "      y_pred = df['pred_label_fixed'].tolist()\n",
        "\n",
        "      df.to_csv(pred_dir + prompt_ver + \"_\" + gpt_model + index_name +'_fixed_label.csv', index=False)\n",
        "\n",
        "\n",
        "      print('----')\n",
        "      classes = tuple(set(y_pred))\n",
        "      print(tuple(set(y_true)))\n",
        "      print(classes)\n",
        "      if len(classes) < 4 and max(classes)==3 :\n",
        "        classes = (0,1,2,3)\n",
        "      print(classes)\n",
        "\n",
        "      recall_list, prec_list,  count_list, num_cls = show_confusionmatrix(y_true,y_pred)\n",
        "\n",
        "      recall_all.append(recall_list)\n",
        "      prec_all.append(prec_list)\n",
        "      count_all.append(count_list)\n",
        "      num_class_all.append(num_cls)\n",
        "\n",
        "      gpt_models_all.append(gpt_model)\n",
        "      prompt_vers_all.append(prompt_ver)\n",
        "\n",
        "      accuracy_all.append(accuracy(y_true,y_pred))\n",
        "\n",
        "    except Exception as error:\n",
        "       print(\"An exception occurred:\", error)\n",
        "\n",
        "out_dir = '/content/drive/My Drive/Data_ClassificationModel/'\n",
        "\n",
        "\n",
        "create_dataframe(recall_all, num_class_all, out_dir+'recall_all.csv', gpt_models_all, prompt_vers_all )\n",
        "create_dataframe(prec_all, num_class_all, out_dir + 'prec_all.csv', gpt_models_all, prompt_vers_all)\n",
        "create_dataframe(count_all, num_class_all,out_dir + 'count_all.csv', gpt_models_all, prompt_vers_all)\n",
        "create_dataframe_acc(accuracy_all, num_class_all, out_dir +'acc_all.csv', gpt_models_all, prompt_vers_all)\n",
        "\n",
        "prec_csv = out_dir + 'prec_all.csv'\n",
        "recall_csv = out_dir + 'recall_all.csv'\n",
        "count_csv = out_dir + 'count_all.csv'\n",
        "\n",
        "output_f1_csv = out_dir + 'f1_score.csv'\n",
        "output_prec_recall_simple_csv =  out_dir + 'prec_recall_simple.csv'\n",
        "\n",
        "\n",
        "compute_f1(prec_csv, recall_csv, count_csv, output_f1_csv, output_prec_recall_simple_csv, num_class=3)\n"
      ],
      "metadata": {
        "id": "a2yDETC3eTGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15df339-e227-45ab-e9d0-bec43d4b893e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Data_ClassificationModel/pure_fewshot_specificCOT_gpt-4_index0_to_110_v0.csv\n",
            "----\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "/content/drive/My Drive/Data_ClassificationModel/pure_fewshot_specificCOT_genCOT_gpt-4_index0_to_110_v0.csv\n",
            "----\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "/content/drive/My Drive/Data_ClassificationModel/pure_fewshot_specificCOT_assertion_gpt-4_index0_to_110_v0.csv\n",
            "----\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "/content/drive/My Drive/Data_ClassificationModel/pure_fewshot_specificCOT_genCOT_assertion_gpt-4_index0_to_110_v0.csv\n",
            "----\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n",
            "(0, 1, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   model                                     prompt  0_perclass_f1  \\\n",
              " 0  gpt-4                   pure_fewshot_specificCOT       0.909091   \n",
              " 1  gpt-4            pure_fewshot_specificCOT_genCOT       0.909091   \n",
              " 2  gpt-4         pure_fewshot_specificCOT_assertion       0.909091   \n",
              " 3  gpt-4  pure_fewshot_specificCOT_genCOT_assertion       0.909091   \n",
              " \n",
              "    0_weighted_f1  1_perclass_f1  1_weighted_f1  2_perclass_f1  2_weighted_f1  \\\n",
              " 0        0.30303       0.615385       0.205128       0.333333       0.111111   \n",
              " 1        0.30303       0.750000       0.250000       0.909091       0.303030   \n",
              " 2        0.30303       0.666667       0.222222       0.800000       0.266667   \n",
              " 3        0.30303       0.750000       0.250000       0.909091       0.303030   \n",
              " \n",
              "    macro_average_skipNA  weighted_average_skipNA  macro_average_NAas0  \\\n",
              " 0              0.619270                 0.619270             0.619270   \n",
              " 1              0.856061                 0.856061             0.856061   \n",
              " 2              0.791919                 0.791919             0.791919   \n",
              " 3              0.856061                 0.856061             0.856061   \n",
              " \n",
              "    weighted_average_NAas0  \n",
              " 0                0.619270  \n",
              " 1                0.856061  \n",
              " 2                0.791919  \n",
              " 3                0.856061  ,\n",
              "    model                                     prompt    0_prec  0_recall  \\\n",
              " 0  gpt-4                   pure_fewshot_specificCOT  0.833333       1.0   \n",
              " 1  gpt-4            pure_fewshot_specificCOT_genCOT  0.833333       1.0   \n",
              " 2  gpt-4         pure_fewshot_specificCOT_assertion  0.833333       1.0   \n",
              " 3  gpt-4  pure_fewshot_specificCOT_genCOT_assertion  0.833333       1.0   \n",
              " \n",
              "    1_prec  1_recall    2_prec  2_recall  \n",
              " 0    0.50       0.8  1.000000       0.2  \n",
              " 1    1.00       0.6  0.833333       1.0  \n",
              " 2    0.75       0.6  0.800000       0.8  \n",
              " 3    1.00       0.6  0.833333       1.0  )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# out_dir = '/content/drive/MyDrive/Classification Model/COT/COT refinement 9_14_2023/Evaluation_ASST1/updated_test_wquestion_9_13_23_ASST1_preds/'\n",
        "# out_dir = '/content/drive/My Drive/Data_ClassificationModel/'\n",
        "\n",
        "# prec_csv = out_dir + 'prec_all.csv'\n",
        "# recall_csv = out_dir + 'recall_all.csv'\n",
        "# count_csv = out_dir + 'count_all.csv'\n",
        "\n",
        "# output_f1_csv = out_dir + 'f1_score.csv'\n",
        "# output_prec_recall_simple_csv =  out_dir + 'prec_recall_simple.csv'\n",
        "\n",
        "# compute_f1(prec_csv, recall_csv, count_csv, output_f1_csv, output_prec_recall_simple_csv, num_class=3)"
      ],
      "metadata": {
        "id": "IZ2nYOQ4-z0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = 'gpt-4'\n",
        "prompt_ver = 'pure_fewshot_specificCOT'\n",
        "\n",
        "pred_dir = '/content/drive/My Drive/Data_ClassificationModel/'\n",
        "\n",
        "pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name + \"_v1\" + '.csv'\n",
        "df_1 = pd.read_csv(pred_path)\n",
        "df_1['ver'] = 'ver1'\n",
        "pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name + \"_v2\" + '.csv'\n",
        "df_2 = pd.read_csv(pred_path)\n",
        "df_2['ver'] = 'ver2'\n",
        "pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name + \"_v3\" + '.csv'\n",
        "df_3 = pd.read_csv(pred_path)\n",
        "df_3['ver'] = 'ver3'\n",
        "\n",
        "df = df_1.append(df_2, ignore_index=True)\n",
        "\n",
        "df = df.append(df_3, ignore_index=True)\n",
        "\n",
        "df = df.sort_values(by=['id_internal'])\n",
        "\n",
        "df.to_csv(pred_dir+'allpred.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob7_lWHGG6BX",
        "outputId": "0cafc1a5-4fc5-4ac6-c293-42dfe2800618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-3980bc6b3738>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df_1.append(df_2, ignore_index=True)\n",
            "<ipython-input-15-3980bc6b3738>:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(df_3, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_model = 'gpt-4'\n",
        "prompt_ver = 'pure_fewshot_specificCOT_genCOT'\n",
        "\n",
        "pred_dir = '/content/drive/My Drive/Data_ClassificationModel/'\n",
        "\n",
        "\n",
        "index_name  = '_index0_to_57'\n",
        "pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name  + '.csv'\n",
        "df_1 = pd.read_csv(pred_path)\n",
        "\n",
        "index_name  = '_index57_to_134'\n",
        "pred_path = pred_dir + prompt_ver + \"_\" + gpt_model + index_name + '.csv'\n",
        "df_2 = pd.read_csv(pred_path)\n",
        "\n",
        "df = df_1.append(df_2, ignore_index=True)\n",
        "\n",
        "index_name  = '_index0_to_134'\n",
        "df.to_csv(pred_dir + prompt_ver + \"_\" + gpt_model + index_name  + '.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8314b5-9050-4923-f5b0-4dddd32d0205",
        "id": "SdroRsrwdOT1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cbf6bbc377b3>:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df_1.append(df_2, ignore_index=True)\n"
          ]
        }
      ]
    }
  ]
}